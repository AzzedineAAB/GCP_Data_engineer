{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfda27e5-d357-4628-b2b9-139fc528ea12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leccture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------+------+-----------+--------------------+------+----------+--------------------+------------+\n",
      "|          ID|     F_Name|L_Name|M_Name|        SSN|         PhoneNumber|Gender|       DOB|             Address|ModifiedDate|\n",
      "+------------+-----------+------+------+-----------+--------------------+------+----------+--------------------+------------+\n",
      "|HOSP1-000001|   Victoria|Gamble|     Q|318-87-5123|          4902994299|  Male|1994-12-24|7912 Arthur Loaf ...|  2021-06-19|\n",
      "|HOSP1-000002|     Meghan|  West|     F|110-08-3049|  (703)210-5078x2916|Female|2009-04-03|70600 Destiny Gro...|  2024-07-06|\n",
      "|HOSP1-000003|Christopher| Klein|     X|528-95-5438|001-234-668-8071x032|  Male|1987-11-15|61743 Vickie Tunn...|  2021-02-22|\n",
      "|HOSP1-000004|      Grace| Young|     T|821-27-2909|        208.792.7266|Female|2015-02-20|1452 Gregory View...|  2021-12-27|\n",
      "|HOSP1-000005|     Andrew| Smith|     N|535-73-4816|  574-505-3750x66300|  Male|1973-06-12|15378 Thomas Lock...|  2020-08-20|\n",
      "+------------+-----------+------+------+-----------+--------------------+------+----------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Connexion réussie!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage, bigquery\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import datetime\n",
    "import json\n",
    "import logging\n",
    "\n",
    "\n",
    "#initialisation de GCS et BQ\n",
    "GCS = storage.Client()\n",
    "BQ = bigquery.Client()\n",
    "\n",
    "#initialisation de la session spark\n",
    "\n",
    "spark = SparkSession.builder.appName(\"HospitalAMySQLToLanding\").getOrCreate()\n",
    "\n",
    "# Google Cloud Storage (GCS) Configuration\n",
    "\n",
    "GCS_BUCKET = \"healthcar032025\"\n",
    "HOPITAL_NAME = \"hopital-b\"\n",
    "LANDING_PATH = f\"gs://{GCS_BUCKET}/landing/{HOPITAL_NAME}/\"\n",
    "ARCHIVE_PATH = f\"gs://{GCS_BUCKET}/landing/{HOPITAL_NAME}/archive/\"\n",
    "CONFIG_FILE_PATH = f\"gs://{GCS_BUCKET}/configs/load_config.csv\"\n",
    "\n",
    "\n",
    "MYSQL_CONFIG = {\n",
    "    \"url\": \"jdbc:mysql://34.28.132.3:3306/hopital-b?useSSL=false&allowPublicKeyRetrieval=true\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\",\n",
    "    \"user\": \"azzedine\",\n",
    "    \"password\": \"ZainaSalem1967@\"\n",
    "}\n",
    "\n",
    "#test de connexion\n",
    "try:\n",
    "   \n",
    "    df = spark.read.jdbc(url=MYSQL_CONFIG[\"url\"], table=\"patients\", properties={\n",
    "                             \"user\": MYSQL_CONFIG[\"user\"],\n",
    "                             \"password\": MYSQL_CONFIG[\"password\"],\n",
    "                             \"driver\": MYSQL_CONFIG[\"driver\"]\n",
    "                         })\n",
    "    print('leccture')\n",
    "   \n",
    "    df.show(5)  \n",
    "    \n",
    "    print(\"Connexion réussie!\")\n",
    "except Exception as e:\n",
    "     logger.error(\"Erreur lors de la connexion à MySQL:\", exc_info=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aff4b9b-17b8-4b93-9c36-c1b09a7022c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#big query confuguration\n",
    "BQ_PROJECT = \"hybrid-ridge-457421-t8\"\n",
    "BQ_AUDIT_TABLE = f\"{BQ_PROJECT}.temp_dataset.audit_log\" # stocker l'etat de chaque chargement\n",
    "BQ_LOG_TABLE = f\"{BQ_PROJECT}.temp_dataset.pipeline_logs\" #stocker les events de l'excustion de la pipline\n",
    "BQ_TEMP_PATH = f\"{GCS_BUCKET}/temp/\"  \n",
    "\n",
    "log_entries = [] \n",
    "\n",
    "def log_event(type_event,message,table=None):\n",
    "    log_entry = {\n",
    "        \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "        \"event_type\": type_event,\n",
    "        \"message\": message,\n",
    "        \"table\": table\n",
    "    }\n",
    "    log_entries.append(log_entry)\n",
    "    print(f\"[{log_entry['timestamp']}] {type_event} - {message}\") \n",
    "    \n",
    "    \n",
    "def save_logs_to_gcs():\n",
    "    \"\"\"Enregistrer les logs dans GCS sous forme de fichier JSON\"\"\"\n",
    "    log_filename = f\"pipeline_log_{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}.json\"\n",
    "    log_filepath = f\"temp/pipeline_logs/{log_filename}\"\n",
    "\n",
    "    json_data = json.dumps(log_entries, indent=4)\n",
    "\n",
    "    # Sauvegarde sur GCS\n",
    "    bucket = GCS.bucket(GCS_BUCKET)\n",
    "    blob = bucket.blob(log_filepath)\n",
    "    blob.upload_from_string(json_data, content_type=\"application/json\")\n",
    "\n",
    "    print(f\"✅ Logs successfully saved to GCS at gs://{GCS_BUCKET}/{log_filepath}\")\n",
    "    \n",
    "\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2294d492-b153-420a-8136-c759e23146ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lastes_watemark(table_name):\n",
    "    query = f\"\"\"\n",
    "        SELECT MAX(load_timestamp) AS latest_timestamp\n",
    "        FROM `{BQ_AUDIT_TABLE}`\n",
    "        WHERE tablename = '{table_name}' and data_source = \"hopital_b_db\"\n",
    "    \"\"\"\n",
    "    query_job = BQ.query(query)\n",
    "    result = query_job.result()\n",
    "    for row in result:\n",
    "        if row.latest_timestamp:\n",
    "            return row.timestamp\n",
    "        else:\n",
    "            return \"1900-01-01 00:00:00\"\n",
    "        \n",
    "    return \"1900-01-01 00:00:00\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9073b821-6c3e-4515-9e1d-b6129ea2249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_save_in_gcs(table_name,load_type,watermark_col):\n",
    "   \n",
    "    \n",
    "    try:\n",
    "        if load_type.lower()=='incremental':\n",
    "            last_watermark=get_lastes_watemark(table_name)\n",
    "            query=f\"\"\"\n",
    "            (SELECT * FROM {table_name} WHERE {watermark_col} > '{last_watermark}') AS t\n",
    "            \"\"\"\n",
    "        else:\n",
    "            query=f\"(SELECT * FROM {table_name}) AS t\"\n",
    "        \n",
    "        df = (spark.read.format(\"jdbc\")\n",
    "                .option(\"url\", MYSQL_CONFIG[\"url\"])\n",
    "                .option(\"user\", MYSQL_CONFIG[\"user\"])\n",
    "                .option(\"password\", MYSQL_CONFIG[\"password\"])\n",
    "                .option(\"driver\", MYSQL_CONFIG[\"driver\"])\n",
    "                .option(\"dbtable\", query)\n",
    "                .load())\n",
    "        log_event(\"SUCCESS\", f\" extract des donnees  reussie de {table}\", table=table)\n",
    "        \n",
    "        today = datetime.datetime.today().strftime('%d%m%Y')\n",
    "        JSON_FILE_PATH = f\"landing/{HOPITAL_NAME}/{table}/{table}_{today}.json\"\n",
    "        \n",
    "        bucket = GCS.bucket(GCS_BUCKET)\n",
    "        blob = bucket.blob(JSON_FILE_PATH)\n",
    "        blob.upload_from_string(df.toPandas().to_json(orient=\"records\", lines=True), content_type=\"application/json\")\n",
    "        \n",
    "        log_event(\"SUCCÈS\", f\" Fichier JSON enregistré avec succès dans gs://{GCS_BUCKET}/{JSON_FILE_PATH}\", table=table)\n",
    "        \n",
    "        #table_audit\n",
    "        audit_df = spark.createDataFrame([\n",
    "            (\"hospital_a_db\", table, load_type, df.count(), datetime.datetime.now(), \"SUCCESS\")], \n",
    "            [\"data_source\", \"tablename\", \"load_type\", \"record_count\", \"load_timestamp\", \"status\"])\n",
    "\n",
    "        (audit_df.write.format(\"bigquery\")\n",
    "            .option(\"table\", BQ_AUDIT_TABLE)\n",
    "            .option(\"temporaryGcsBucket\", GCS_BUCKET)\n",
    "            .mode(\"append\")\n",
    "            .save())\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_event(\"ERREUR\", f\"Erreur lors du traitement de la table {table} : {str(e)}\", table=table)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed55e58b-148c-47d4-bd09-e229cb22422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_save_in_gcs(table_name,load_type,watermark_col):\n",
    "   \n",
    "    \n",
    "    try:\n",
    "        if load_type.lower()=='incremental':\n",
    "            last_watermark=get_lastes_watemark(table_name)\n",
    "            query=f\"\"\"\n",
    "            (SELECT * FROM {table_name} WHERE {watermark_col} > '{last_watermark}') AS t\n",
    "            \"\"\"\n",
    "        else:\n",
    "            query=f\"(SELECT * FROM {table_name}) AS t\"\n",
    "        \n",
    "        df = (spark.read.format(\"jdbc\")\n",
    "                .option(\"url\", MYSQL_CONFIG[\"url\"])\n",
    "                .option(\"user\", MYSQL_CONFIG[\"user\"])\n",
    "                .option(\"password\", MYSQL_CONFIG[\"password\"])\n",
    "                .option(\"driver\", MYSQL_CONFIG[\"driver\"])\n",
    "                .option(\"dbtable\", query)\n",
    "                .load())\n",
    "        log_event(\"SUCCESS\", f\" extract des donnees  reussie de {table}\", table=table)\n",
    "        \n",
    "        today = datetime.datetime.today().strftime('%d%m%Y')\n",
    "        JSON_FILE_PATH = f\"landing/{HOPITAL_NAME}/{table}/{table}_{today}.json\"\n",
    "        \n",
    "        bucket = GCS.bucket(GCS_BUCKET)\n",
    "        blob = bucket.blob(JSON_FILE_PATH)\n",
    "        blob.upload_from_string(df.toPandas().to_json(orient=\"records\", lines=True), content_type=\"application/json\")\n",
    "        \n",
    "        log_event(\"SUCCÈS\", f\" Fichier JSON enregistré avec succès dans gs://{GCS_BUCKET}/{JSON_FILE_PATH}\", table=table)\n",
    "        \n",
    "        #table_audit\n",
    "        audit_df = spark.createDataFrame([\n",
    "            (\"hospital_a_db\", table, load_type, df.count(), datetime.datetime.now(), \"SUCCESS\")], \n",
    "            [\"data_source\", \"tablename\", \"load_type\", \"record_count\", \"load_timestamp\", \"status\"])\n",
    "\n",
    "        (audit_df.write.format(\"bigquery\")\n",
    "            .option(\"table\", BQ_AUDIT_TABLE)\n",
    "            .option(\"temporaryGcsBucket\", GCS_BUCKET)\n",
    "            .mode(\"append\")\n",
    "            .save())\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_event(\"ERREUR\", f\"Erreur lors du traitement de la table {table} : {str(e)}\", table=table)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54c6a964-20c2-4d59-b357-461ff6778f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_existing_files_to_archive(table):\n",
    "    blobs = list(GCS.bucket(GCS_BUCKET).list_blobs(prefix=f\"landing/{HOPITAL_NAME}/{table}/\"))\n",
    "    existing_files = [blob.name for blob in blobs if blob.name.endswith(\".json\")]\n",
    "    \n",
    "    if not existing_files:\n",
    "        log_event(\"INFO\", f\"Aucun fichier existant pour la table {table}\")\n",
    "        return\n",
    "    \n",
    "    for file in existing_files:\n",
    "        source_blob = GCS.bucket(GCS_BUCKET).blob(file)\n",
    "\n",
    "        # Extract Date from File Name\n",
    "        date_part = file.split(\"_\")[-1].split(\".\")[0]\n",
    "        year, month, day = date_part[-4:], date_part[2:4], date_part[:2]\n",
    "\n",
    "        # Move to Archive\n",
    "        archive_path = f\"landing/{HOPITAL_NAME}/archive/{table}/{year}/{month}/{day}/{file.split('/')[-1]}\"\n",
    "        destination_blob = GCS.bucket(GCS_BUCKET).blob(archive_path)\n",
    "\n",
    "        # Copy file to archive and delete original\n",
    "        GCS.bucket(GCS_BUCKET).copy_blob(source_blob, GCS.bucket(GCS_BUCKET), destination_blob.name)\n",
    "        source_blob.delete()\n",
    "\n",
    "        log_event(\"INFO\", f\"Fichier {file} déplacé vers {archive_path}\", table=table)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaaa5c7c-2ae4-4932-864c-5cd1e913bd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-04T17:38:39.940829] INFO - ✅ Fichier de configuration lu avec succès\n",
      "[2025-05-04T17:38:40.304421] INFO - Aucun fichier existant pour la table encounters\n",
      "[2025-05-04T17:38:41.539800] SUCCESS -  extract des donnees  reussie de encounters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-04T17:38:43.080956] SUCCÈS -  Fichier JSON enregistré avec succès dans gs://healthcar032025/landing/hopital-b/encounters/encounters_04052025.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-04T17:38:58.560719] INFO - Aucun fichier existant pour la table patients\n",
      "[2025-05-04T17:38:59.573377] SUCCESS -  extract des donnees  reussie de patients\n",
      "[2025-05-04T17:39:00.518748] SUCCÈS -  Fichier JSON enregistré avec succès dans gs://healthcar032025/landing/hopital-b/patients/patients_04052025.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-04T17:39:13.146882] INFO - Aucun fichier existant pour la table transactions\n",
      "[2025-05-04T17:39:14.151096] SUCCESS -  extract des donnees  reussie de transactions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-04T17:39:17.408465] SUCCÈS -  Fichier JSON enregistré avec succès dans gs://healthcar032025/landing/hopital-b/transactions/transactions_04052025.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-04T17:39:24.005249] INFO - Aucun fichier existant pour la table providers\n",
      "[2025-05-04T17:39:24.260863] SUCCESS -  extract des donnees  reussie de providers\n",
      "[2025-05-04T17:39:24.699963] SUCCÈS -  Fichier JSON enregistré avec succès dans gs://healthcar032025/landing/hopital-b/providers/providers_04052025.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-04T17:39:31.119220] INFO - Aucun fichier existant pour la table departments\n",
      "[2025-05-04T17:39:31.362525] SUCCESS -  extract des donnees  reussie de departments\n",
      "[2025-05-04T17:39:31.841223] SUCCÈS -  Fichier JSON enregistré avec succès dans gs://healthcar032025/landing/hopital-b/departments/departments_04052025.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Logs successfully saved to GCS at gs://healthcar032025/temp/pipeline_logs/pipeline_log_20250504174009.json\n"
     ]
    }
   ],
   "source": [
    "def read_config_file():\n",
    "    df = spark.read.csv(CONFIG_FILE_PATH, header=True)\n",
    "    log_event(\"INFO\", \"✅ Fichier de configuration lu avec succès\")\n",
    "    return df\n",
    "\n",
    "# read config file\n",
    "config_df = read_config_file()\n",
    "\n",
    "for row in config_df.collect():\n",
    "    if row[\"is_active\"] == '1' and row[\"datasource\"] == \"hospital_b_db\": \n",
    "        db, src, table, load_type, watermark, _, targetpath = row\n",
    "        move_existing_files_to_archive(table)\n",
    "        extract_and_save_in_gcs(table, load_type, watermark)\n",
    "        \n",
    "save_logs_to_gcs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b28b8f-ad9f-4269-8fcb-9bb6d1560cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}