{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f360ecb-824c-4c29-ade6-82005ed46fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leccture\n",
      "+------------+---------+--------+----------+-----------+--------------------+------+----------+--------------------+------------+\n",
      "|   PatientID|FirstName|LastName|MiddleName|        SSN|         PhoneNumber|Gender|       DOB|             Address|ModifiedDate|\n",
      "+------------+---------+--------+----------+-----------+--------------------+------+----------+--------------------+------------+\n",
      "|HOSP1-000001|     Rick|   Russo|         U|188-23-9828|+1-630-829-7585x0769|Female|1937-06-04|Unit 0915 Box 706...|  2020-05-25|\n",
      "|HOSP1-000002|  Gregory|  Graham|         B|730-45-8217|  456.746.7289x69233|Female|1937-06-10|9864 Gibson Islan...|  2021-06-05|\n",
      "|HOSP1-000003|     Mary|    Ryan|         H|348-14-7947|        522-501-5461|Female|1926-08-09|6194 Joseph Turnp...|  2024-09-06|\n",
      "|HOSP1-000004|   Daniel|   Brown|         D|013-38-1645|     +1-345-608-9409|  Male|1971-10-23|780 Conrad Isle, ...|  2022-04-07|\n",
      "|HOSP1-000005|     Brad| Carroll|         M|461-53-6290|   963.994.2969x6232|  Male|1927-10-18|3167 Hall Burg, T...|  2022-06-19|\n",
      "+------------+---------+--------+----------+-----------+--------------------+------+----------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Connexion réussie!\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage, bigquery\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import datetime\n",
    "import json\n",
    "import logging\n",
    "\n",
    "\n",
    "#initialisation de GCS et BQ\n",
    "GCS = storage.Client()\n",
    "BQ = bigquery.Client()\n",
    "\n",
    "#initialisation de la session spark\n",
    "\n",
    "spark = SparkSession.builder.appName(\"HospitalAMySQLToLanding\").getOrCreate()\n",
    "\n",
    "# Google Cloud Storage (GCS) Configuration\n",
    "\n",
    "GCS_BUCKET = \"healthcar032025\"\n",
    "HOPITAL_NAME = \"hopital-a\"\n",
    "LANDING_PATH = f\"gs://{GCS_BUCKET}/landing/{HOPITAL_NAME}/\"\n",
    "ARCHIVE_PATH = f\"gs://{GCS_BUCKET}/landing/{HOPITAL_NAME}/archive/\"\n",
    "CONFIG_FILE_PATH = f\"gs://{GCS_BUCKET}/configs/load_config.csv\"\n",
    "\n",
    "\n",
    "MYSQL_CONFIG = {\n",
    "    \"url\": \"jdbc:mysql://34.56.4.219:3306/hopital_a_db?useSSL=false&allowPublicKeyRetrieval=true\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\",\n",
    "    \"user\": \"azzedine\",\n",
    "    \"password\": \"ZainaSalem1967@\"\n",
    "}\n",
    "\n",
    "#test de connexion\n",
    "try:\n",
    "   \n",
    "    df = spark.read.jdbc(url=MYSQL_CONFIG[\"url\"], table=\"patients\", properties={\n",
    "                             \"user\": MYSQL_CONFIG[\"user\"],\n",
    "                             \"password\": MYSQL_CONFIG[\"password\"],\n",
    "                             \"driver\": MYSQL_CONFIG[\"driver\"]\n",
    "                         })\n",
    "    print('leccture')\n",
    "   \n",
    "    df.show(5)  \n",
    "    \n",
    "    print(\"Connexion réussie!\")\n",
    "except Exception as e:\n",
    "     logger.error(\"Erreur lors de la connexion à MySQL:\", exc_info=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a007b6a-6fa9-495e-8cb4-f63e9e9e0801",
   "metadata": {},
   "outputs": [],
   "source": [
    "#big query confuguration\n",
    "BQ_PROJECT = \"hybrid-ridge-457421-t8\"\n",
    "BQ_AUDIT_TABLE = f\"{BQ_PROJECT}.temp_dataset.audit_log\" # stocker l'etat de chaque chargement\n",
    "BQ_LOG_TABLE = f\"{BQ_PROJECT}.temp_dataset.pipeline_logs\" #stocker les events de l'excustion de la pipline\n",
    "BQ_TEMP_PATH = f\"{GCS_BUCKET}/temp/\"  \n",
    "\n",
    "log_entries = [] \n",
    "\n",
    "def log_event(type_event,message,table=None):\n",
    "    log_entry = {\n",
    "        \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "        \"event_type\": type_event,\n",
    "        \"message\": message,\n",
    "        \"table\": table\n",
    "    }\n",
    "    log_entries.append(log_entry)\n",
    "    print(f\"[{log_entry['timestamp']}] {type_event} - {message}\") \n",
    "    \n",
    "    \n",
    "def save_logs_to_gcs():\n",
    "    \"\"\"Enregistrer les logs dans GCS sous forme de fichier JSON\"\"\"\n",
    "    log_filename = f\"pipeline_log_{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}.json\"\n",
    "    log_filepath = f\"temp/pipeline_logs/{log_filename}\"\n",
    "\n",
    "    json_data = json.dumps(log_entries, indent=4)\n",
    "\n",
    "    # Sauvegarde sur GCS\n",
    "    bucket = GCS.bucket(GCS_BUCKET)\n",
    "    blob = bucket.blob(log_filepath)\n",
    "    blob.upload_from_string(json_data, content_type=\"application/json\")\n",
    "\n",
    "    print(f\"✅ Logs successfully saved to GCS at gs://{GCS_BUCKET}/{log_filepath}\")\n",
    "    \n",
    "\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "415c7982-245d-472c-9458-6e032a0d329a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lastes_watemark(table_name):\n",
    "    query = f\"\"\"\n",
    "        SELECT MAX(load_timestamp) AS latest_timestamp\n",
    "        FROM `{BQ_AUDIT_TABLE}`\n",
    "        WHERE tablename = '{table_name}' and data_source = \"hopital_a_db\"\n",
    "    \"\"\"\n",
    "    query_job = BQ.query(query)\n",
    "    result = query_job.result()\n",
    "    for row in result:\n",
    "        if row.latest_timestamp:\n",
    "            return row.timestamp\n",
    "        else:\n",
    "            return \"1900-01-01 00:00:00\"\n",
    "        \n",
    "    return \"1900-01-01 00:00:00\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06c5185e-4171-49d7-b5da-2d77243a825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_save_in_gcs(table_name,load_type,watermark_col):\n",
    "   \n",
    "    \n",
    "    try:\n",
    "        if load_type.lower()=='incremental':\n",
    "            last_watermark=get_lastes_watemark(table_name)\n",
    "            query=f\"\"\"\n",
    "            (SELECT * FROM {table_name} WHERE {watermark_col} > '{last_watermark}') AS t\n",
    "            \"\"\"\n",
    "        else:\n",
    "            query=f\"(SELECT * FROM {table_name}) AS t\"\n",
    "        \n",
    "        df = (spark.read.format(\"jdbc\")\n",
    "                .option(\"url\", MYSQL_CONFIG[\"url\"])\n",
    "                .option(\"user\", MYSQL_CONFIG[\"user\"])\n",
    "                .option(\"password\", MYSQL_CONFIG[\"password\"])\n",
    "                .option(\"driver\", MYSQL_CONFIG[\"driver\"])\n",
    "                .option(\"dbtable\", query)\n",
    "                .load())\n",
    "        log_event(\"SUCCESS\", f\" extract des donnees  reussie de {table}\", table=table)\n",
    "        \n",
    "        today = datetime.datetime.today().strftime('%d%m%Y')\n",
    "        JSON_FILE_PATH = f\"landing/{HOPITAL_NAME}/{table}/{table}_{today}.json\"\n",
    "        \n",
    "        bucket = GCS.bucket(GCS_BUCKET)\n",
    "        blob = bucket.blob(JSON_FILE_PATH)\n",
    "        blob.upload_from_string(df.toPandas().to_json(orient=\"records\", lines=True), content_type=\"application/json\")\n",
    "        \n",
    "        log_event(\"SUCCÈS\", f\" Fichier JSON enregistré avec succès dans gs://{GCS_BUCKET}/{JSON_FILE_PATH}\", table=table)\n",
    "        \n",
    "        #table_audit\n",
    "        audit_df = spark.createDataFrame([\n",
    "            (\"hospital_a_db\", table, load_type, df.count(), datetime.datetime.now(), \"SUCCESS\")], \n",
    "            [\"data_source\", \"tablename\", \"load_type\", \"record_count\", \"load_timestamp\", \"status\"])\n",
    "\n",
    "        (audit_df.write.format(\"bigquery\")\n",
    "            .option(\"table\", BQ_AUDIT_TABLE)\n",
    "            .option(\"temporaryGcsBucket\", GCS_BUCKET)\n",
    "            .mode(\"append\")\n",
    "            .save())\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_event(\"ERREUR\", f\"Erreur lors du traitement de la table {table} : {str(e)}\", table=table)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8408cdb5-38fe-4280-aa0f-dde9111b3b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_existing_files_to_archive(table):\n",
    "    blobs = list(GCS.bucket(GCS_BUCKET).list_blobs(prefix=f\"landing/{HOPITAL_NAME}/{table}/\"))\n",
    "    existing_files = [blob.name for blob in blobs if blob.name.endswith(\".json\")]\n",
    "    \n",
    "    if not existing_files:\n",
    "        log_event(\"INFO\", f\"Aucun fichier existant pour la table {table}\")\n",
    "        return\n",
    "    \n",
    "    for file in existing_files:\n",
    "        source_blob = GCS.bucket(GCS_BUCKET).blob(file)\n",
    "\n",
    "        # Extract Date from File Name\n",
    "        date_part = file.split(\"_\")[-1].split(\".\")[0]\n",
    "        year, month, day = date_part[-4:], date_part[2:4], date_part[:2]\n",
    "\n",
    "        # Move to Archive\n",
    "        archive_path = f\"landing/{HOPITAL_NAME}/archive/{table}/{year}/{month}/{day}/{file.split('/')[-1]}\"\n",
    "        destination_blob = GCS.bucket(GCS_BUCKET).blob(archive_path)\n",
    "\n",
    "        # Copy file to archive and delete original\n",
    "        GCS.bucket(GCS_BUCKET).copy_blob(source_blob, GCS.bucket(GCS_BUCKET), destination_blob.name)\n",
    "        source_blob.delete()\n",
    "\n",
    "        log_event(\"INFO\", f\"Fichier {file} déplacé vers {archive_path}\", table=table)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8de7521d-2912-45c6-8ecf-5a83e573246f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-04T17:19:00.673663] INFO - ✅ Fichier de configuration lu avec succès\n",
      "[2025-05-04T17:19:00.862381] INFO - Aucun fichier existant pour la table encounters\n",
      "[2025-05-04T17:19:02.197242] SUCCESS -  extract des donnees  reussie de encounters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-04T17:19:03.122841] SUCCÈS -  Fichier JSON enregistré avec succès dans gs://healthcar032025/landing/hopital-a/encounters/encounters_04052025.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-04T17:19:11.281347] INFO - Aucun fichier existant pour la table patients\n",
      "[2025-05-04T17:19:12.412858] SUCCESS -  extract des donnees  reussie de patients\n",
      "[2025-05-04T17:19:13.158620] SUCCÈS -  Fichier JSON enregistré avec succès dans gs://healthcar032025/landing/hopital-a/patients/patients_04052025.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-04T17:19:19.562205] INFO - Aucun fichier existant pour la table transactions\n",
      "[2025-05-04T17:19:20.580737] SUCCESS -  extract des donnees  reussie de transactions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-04T17:19:21.840479] SUCCÈS -  Fichier JSON enregistré avec succès dans gs://healthcar032025/landing/hopital-a/transactions/transactions_04052025.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-04T17:19:27.544239] INFO - Fichier landing/hopital-a/providers/providers_04052025.json déplacé vers landing/hopital-a/archive/providers/2025/05/04/providers_04052025.json\n",
      "[2025-05-04T17:19:27.776329] SUCCESS -  extract des donnees  reussie de providers\n",
      "[2025-05-04T17:19:28.119657] SUCCÈS -  Fichier JSON enregistré avec succès dans gs://healthcar032025/landing/hopital-a/providers/providers_04052025.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-04T17:19:33.680377] INFO - Fichier landing/hopital-a/departments/departments_04052025.json déplacé vers landing/hopital-a/archive/departments/2025/05/04/departments_04052025.json\n",
      "[2025-05-04T17:19:33.921582] SUCCESS -  extract des donnees  reussie de departments\n",
      "[2025-05-04T17:19:34.270278] SUCCÈS -  Fichier JSON enregistré avec succès dans gs://healthcar032025/landing/hopital-a/departments/departments_04052025.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Logs successfully saved to GCS at gs://healthcar032025/temp/pipeline_logs/pipeline_log_20250504171940.json\n"
     ]
    }
   ],
   "source": [
    "def read_config_file():\n",
    "    df = spark.read.csv(CONFIG_FILE_PATH, header=True)\n",
    "    log_event(\"INFO\", \"✅ Fichier de configuration lu avec succès\")\n",
    "    return df\n",
    "\n",
    "# read config file\n",
    "config_df = read_config_file()\n",
    "\n",
    "for row in config_df.collect():\n",
    "    if row[\"is_active\"] == '1' and row[\"datasource\"] == \"hospital_a_db\": \n",
    "        db, src, table, load_type, watermark, _, targetpath = row\n",
    "        move_existing_files_to_archive(table)\n",
    "        extract_and_save_in_gcs(table, load_type, watermark)\n",
    "        \n",
    "save_logs_to_gcs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb636ce8-f579-441a-99be-1524d204d149",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}